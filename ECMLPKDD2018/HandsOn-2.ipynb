{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficiency/Effectiveness Trade-offs in Learning to Rank\n",
    "### Tutorial @ ECML PKDD 2018, HandsOn Session N. 2\n",
    "\n",
    "##### Claudio Lucchese (UniVe), Franco Maria Nardini (ISTI-CNR)\n",
    "##### High Performance Computing Lab. http://hpc.isti.cnr.it/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hpc.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.model import RTEnsemble\n",
    "from rankeval.metrics import NDCG\n",
    "from rankeval.analysis.effectiveness import tree_wise_performance\n",
    "from rankeval.visualization.effectiveness import plot_tree_wise_performance\n",
    "from rankeval.analysis.effectiveness import model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda\n",
    "\n",
    " 0. Train the optimal **LambdaMart** model and evaluate its effectiveness and efficiency\n",
    " 0. Removing and optimizing trees with **Cleaver**\n",
    " 0. Improving effectiveness with **DART**\n",
    " 0. Improving efficiency with **X-Dart**\n",
    " 0. From cond-op/Vpred to **QuickScorer**\n",
    " 0. From QuickScorer to **V-QuickScorer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    " - Quickrank, see installation instructions. https://github.com/hpclab/quickrank\n",
    " - Install Quickscorer (private HPC git) master branch under NDA. http://learningtorank.isti.cnr.it/\n",
    " - Install RankEval, see installation instructions. https://github.com/hpclab/rankeval \n",
    " - Download the Istella-S LETOR dataset. http://blog.istella.it/istella-learning-to-rank-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Options\n",
    "\n",
    "# paths to executable files\n",
    "QUICKRANK = \"./quickrank/bin/quicklearn\"\n",
    "SCORER    = \"./quickrank/bin/quickscore\"\n",
    "\n",
    "QUICKSCORER = \"./QuickScorer/bin/quickscorer\"\n",
    "QUICKSCORER_GPU = \"./QuickScorer-GPU/GPUQS/bin/quickscorer\"\n",
    "\n",
    "# paths to Istella-S dataset\n",
    "train_dataset_file = \"/data/letor-datasets/tiscali/sample/ramfs/train.txt\"\n",
    "valid_dataset_file = \"/data/letor-datasets/tiscali/sample/ramfs/vali.txt\"\n",
    "test_dataset_file  = \"/data/letor-datasets/tiscali/sample/ramfs/test.txt\"\n",
    "\n",
    "# paths to model file\n",
    "models_folder            = \"models\"\n",
    "baseline_model_file      = os.path.join(models_folder, \"istella-small.lamdamart.xml\")\n",
    "cleaver_model_file       = os.path.join(models_folder, \"istella-small.lamdamart.cleaver.xml\")\n",
    "\n",
    "dart_model_file          = os.path.join(models_folder, \"istella-small.dart.xml\")\n",
    "xdart_model_file         = os.path.join(models_folder, \"istella-small.xdart.xml\")\n",
    "\n",
    "small_dart_model_file    = os.path.join(models_folder, \"istella-small.dart.small.xml\")\n",
    "small_xdart_model_file   = os.path.join(models_folder, \"istella-small.xdart.small.xml\")\n",
    "\n",
    "# setting floating point precision of Pandas\n",
    "pd.set_option('precision', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the optimal LambdaMart model and evaluate its effectiveness and efficiency\n",
    "\n",
    "[LambdaMart] can be considered the state of the art algorithm.\n",
    "\n",
    "**Note:** As training takes a lot of time, we already trained a model for you and made it available at http://learningtorank.isti.cnr.it/models/istella-small_models.tar.gz so that you can skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!{QUICKRANK} \\\n",
    "    --train {train_dataset_file} \\\n",
    "    --valid {valid_dataset_file} \\\n",
    "    --model-out {baseline_model_file} \\\n",
    "    --num-trees 1500 \\\n",
    "    --num-leaves 64 \\\n",
    "    --shrinkage 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the effectiveness of the model on train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load istella-S dataset\n",
    "train_dataset = Dataset.load(train_dataset_file, name=\"train\")\n",
    "valid_dataset = Dataset.load(valid_dataset_file, name=\"valid\")\n",
    "test_dataset  = Dataset.load(test_dataset_file, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "baseline_model = RTEnsemble(baseline_model_file, name=\"LambdaMart\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric\n",
    "ndcg_10 = NDCG(cutoff=10, \n",
    "               no_relevant_results=0.0) # assign score 0 to queries without relevant docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure NDCG every 20 trees\n",
    "tree_wise_perf = tree_wise_performance( datasets =[train_dataset, valid_dataset, test_dataset], \n",
    "                                        models   =[baseline_model],\n",
    "                                        metrics  =[ndcg_10],\n",
    "                                        step=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_list = plot_tree_wise_performance(tree_wise_perf, compare = \"datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_wise_perf.loc[{'dataset':test_dataset}].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_effectiveness = tree_wise_perf.loc[{'dataset':test_dataset,\n",
    "                                             'model':baseline_model, \n",
    "                                             'metric':ndcg_10}].values[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Con-Op based C code as a baseline for the scoring time evaluation\n",
    "\n",
    "def run_condop(model_file, dataset_file, rounds=1):\n",
    "    condop_source = model_file + \".c\"\n",
    "    condop_compiled = model_file + \".bin\"\n",
    "\n",
    "    # create the C code\n",
    "    print (\" 1. Creating the C code for \" + model_file)\n",
    "    \n",
    "    _ = !{QUICKRANK} \\\n",
    "      --generator condop \\\n",
    "      --model-file {model_file} \\\n",
    "      --code-file {condop_source}\n",
    "    \n",
    "    # Compile an executable ranker. The resulting ranker is SCORER=./quickrank/bin/quickscore\n",
    "    print (\" 2. Compiling the model\")\n",
    "\n",
    "    # actually compule only if the model is newer\n",
    "    if ( not os.path.exists(condop_compiled) or \n",
    "         os.path.getmtime(condop_compiled)<os.path.getmtime(baseline_model_file) ):\n",
    "        # replace empty scorer\n",
    "        !cp {condop_source} ./quickrank/src/scoring/ranker.cc\n",
    "        # compile\n",
    "        _ = !make -j -C ./quickrank/build_ quickscore \n",
    "        # copy compiled scorer\n",
    "        !cp {SCORER} {condop_compiled}\n",
    "    \n",
    "    # Run the compiled model\n",
    "    print (\" 3. Running the compiled model\")\n",
    "    scorer_out = !{condop_compiled} \\\n",
    "      -d {dataset_file} \\\n",
    "      -r {rounds}\n",
    "    \n",
    "    print (scorer_out.n)\n",
    "    \n",
    "    # takes the scoring time in milli-seconds\n",
    "    scoring_time = float(scorer_out.l[-1].split()[-2])* 10**6\n",
    "    \n",
    "    return scoring_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_efficiency = run_condop(baseline_model_file, test_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store current results\n",
    "results = pd.DataFrame(columns=['Model', '# Trees', 'NDCG@10', 'Doc. Scoring Time µs.'])\n",
    "\n",
    "results.loc[len(results)] = [baseline_model.name, baseline_model.n_trees, baseline_effectiveness, baseline_efficiency]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Pruning and optimizing trees with Cleaver\n",
    "\n",
    "[Cleaver] is a pruning algorithm removes the trees in a given forest that contribute less and fine-tunes the weights of the remaining ones. Here we apply such pruning to the LambdaMart model just built. \n",
    "\n",
    "<img src=\"images/cleaver.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the effectiveness of the model\n",
    "\n",
    "Cleaver is implemented by QuickRank. See documentation here: https://github.com/hpclab/quickrank/blob/master/documentation/cleaver.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_rate = 0.5\n",
    "!{QUICKRANK}\\\n",
    "    --model-in {baseline_model_file} \\\n",
    "    --train {train_dataset_file} \\\n",
    "    --valid {valid_dataset_file} \\\n",
    "    --opt-algo CLEAVER \\\n",
    "    --opt-method QUALITY_LOSS_ADV \\\n",
    "    --pruning-rate {pruning_rate} \\\n",
    "    --with-line-search \\\n",
    "    --num-samples 20 \\\n",
    "    --window-size 10 \\\n",
    "    --reduction-factor 0.95 \\\n",
    "    --max-iterations 100 \\\n",
    "    --max-failed-valid 20 \\\n",
    "    --adaptive \\\n",
    "    --opt-algo-model {cleaver_model_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaver_model = RTEnsemble(cleaver_model_file, name=\"Cleaver\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# measure NDCG\n",
    "tree_wise_perf = tree_wise_performance(datasets=[test_dataset], \n",
    "                                        models=[baseline_model, cleaver_model],\n",
    "                                        metrics=[ndcg_10],\n",
    "                                        step=20)\n",
    "fig_list = plot_tree_wise_performance(tree_wise_perf, compare = \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_perf = model_performance(datasets=[test_dataset], \n",
    "                                 models=[cleaver_model, baseline_model], \n",
    "                                 metrics=[ndcg_10])\n",
    "models_perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import statistical_significance\n",
    "stat_sig = statistical_significance(datasets=[test_dataset],\n",
    "                                    model_a=cleaver_model, model_b=baseline_model, \n",
    "                                    metrics=[ndcg_10],\n",
    "                                    n_perm=100000 )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude difference is not statistically significant. The model built by Cleaver can be used instead of the LambdaMart model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaver_effectiveness = float( models_perf.loc[{'model':cleaver_model, \n",
    "                                                'dataset':test_dataset, \n",
    "                                                'metric':ndcg_10}] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaver_efficiency = run_condop(cleaver_model_file, test_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [cleaver_model.name, cleaver_model.n_trees, cleaver_effectiveness, cleaver_efficiency]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving effectiveness with DART\n",
    "\n",
    "[Dart] expoits a *dropout* strategy temporarily muting trees during training. Dart is thus able to produce higher quality models. Our goal is to build a smaller model providing the same quality as the baseline model.\n",
    "\n",
    "<img src=\"images/dart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Evaluate the effectiveness of the model\n",
    "\n",
    "Dart is implemented by QuickRank. See documentation here: https://github.com/hpclab/quickrank/blob/master/documentation/xdart.md.\n",
    "\n",
    "**Note:** As training takes a lot of time, we already trained a model for you and made it available at http://learningtorank.isti.cnr.it/models/istella-small_models.tar.gz so that you can skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train Dart with QuickRank\n",
    "!{QUICKRANK} \\\n",
    "  --algo DART \\\n",
    "  --train {train_dataset_file} \\\n",
    "  --valid {valid_dataset_file} \\\n",
    "  --model-out {dart_model_file} \\\n",
    "  --num-trees 1500 \\\n",
    "  --num-leaves 64 \\\n",
    "  --shrinkage 1.0 \\\n",
    "  --sample-type UNIFORM \\\n",
    "  --normalize-type TREE \\\n",
    "  --adaptive-type FIXED \\\n",
    "  --rate-drop 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Dart Models\n",
    "dart_model = RTEnsemble(dart_model_file, name=\"Dart\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure NDCG\n",
    "tree_wise_perf = tree_wise_performance(datasets=[test_dataset], \n",
    "                                        models=[baseline_model, cleaver_model, dart_model],\n",
    "                                        metrics=[ndcg_10],\n",
    "                                        step=20)\n",
    "fig_list = plot_tree_wise_performance(tree_wise_perf, compare = \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want at least the same effectiveness as the baseline model\n",
    "tree_wise_perf.loc[{'model':dart_model, 'k':range(700,800,20)}].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dart_trees = 760\n",
    "small_dart = dart_model.copy(n_trees=small_dart_trees)\n",
    "small_dart.save(small_dart_model_file)\n",
    "\n",
    "small_dart_effectiveness = float( tree_wise_perf.loc[{'model':dart_model, 'k':small_dart_trees, 'metric':ndcg_10}].values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_dart_efficiency = run_condop(small_dart_model_file, test_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [small_dart.name, small_dart.n_trees, small_dart_effectiveness, small_dart_efficiency]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving efficiency with X-Dart\n",
    "\n",
    "[XDart] extends the idea of Dart by allowing the permanent removal of trees: muted dropout trees are permanently removed if they are outperformed by a single newly learnt tree.\n",
    "\n",
    "<img src=\"images/xdart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the effectiveness of the model\n",
    "\n",
    "X-Dart is implemented by QuickRank. See documentation here: https://github.com/hpclab/quickrank/blob/master/documentation/xdart.md.\n",
    "\n",
    "**Note:** As training takes a lot of time, we already trained a model for you and made it available at http://learningtorank.isti.cnr.it/models/istella-small_models.tar.gz so that you can skip the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train X-Dart with QuickRank\n",
    "!{QUICKRANK} \\\n",
    "  --algo DART \\\n",
    "  --train {train_dataset_file} \\\n",
    "  --valid {valid_dataset_file} \\\n",
    "  --model-out {xdart_model_file} \\\n",
    "  --num-trees 1500 \\\n",
    "  --num-leaves 64 \\\n",
    "  --shrinkage 1.0 \\\n",
    "  --sample-type UNIFORM \\\n",
    "  --normalize-type TREE \\\n",
    "  --adaptive-type PLUSHALF_RESET_LB1_UB5 \\\n",
    "  --rate-drop 1.0 \\\n",
    "  --keep-drop \\\n",
    "  --drop-on-best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdart_model = RTEnsemble(xdart_model_file, name=\"X-Dart\", format=\"QuickRank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# measure NDCG\n",
    "tree_wise_perf = tree_wise_performance(datasets=[test_dataset], \n",
    "                                        models=[baseline_model, cleaver_model, dart_model, xdart_model],\n",
    "                                        metrics=[ndcg_10],\n",
    "                                        step=20)\n",
    "fig_list = plot_tree_wise_performance(tree_wise_perf, compare = \"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want at least the same effectiveness as the baseline model\n",
    "tree_wise_perf.loc[{'model':xdart_model, 'k':range(500,720,20)}].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_xdart_trees = 560\n",
    "small_xdart = xdart_model.copy(n_trees=small_xdart_trees)\n",
    "small_xdart.save(small_xdart_model_file)\n",
    "\n",
    "small_xdart_effectiveness = float( tree_wise_perf.loc[{'model':xdart_model, 'k':small_xdart_trees, 'metric':ndcg_10}].values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the efficiency of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_xdart_efficiency = run_condop(small_xdart_model_file, test_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [small_xdart.name, small_xdart.n_trees, small_xdart_effectiveness, small_xdart_efficiency]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting QuickScorer to speed-up the scoring time\n",
    "\n",
    "[QuickScorer] uses a novel traversal methods and a cache-friendly data layout that reduces dramatically the traversal time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model_file = small_xdart_model_file\n",
    "target_model = small_xdart\n",
    "target_model.name = \"QuickScorer\"\n",
    "target_model_effectiveness = small_xdart_effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_out = !{QUICKSCORER} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {target_model_file} \\\n",
    "  -l 64 \\\n",
    "  -r 1 \\\n",
    "  -t 0\n",
    "\n",
    "# -l : max number of leaves\n",
    "# -r : rounds\n",
    "# -t : tree type (e.g. oblivious)\n",
    "    \n",
    "print (scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "scoring_time = float(scorer_out.l[-1].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[len(results)] = [target_model.name, target_model.n_trees, target_model_effectiveness, scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting QuickScorer to speed-up the scoring time\n",
    "\n",
    "[V-QuickScorer] improves over QuickScorer by exploiting 256-bits wide CPU registers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer_out = !{QUICKSCORER} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {target_model_file} \\\n",
    "  -l 64 \\\n",
    "  -r 1 \\\n",
    "  -t 3 \\\n",
    "  -v 8 \\\n",
    "  --avx\n",
    "\n",
    "# -v : 8 docs in parallel\n",
    "\n",
    "print (scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "scoring_time = float(scorer_out.l[-1].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[len(results)] = ['V-QS', target_model.n_trees, target_model_effectiveness, scoring_time]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"Speed-up\"] = results[\"Doc. Scoring Time µs.\"][0]/ results[\"Doc. Scoring Time µs.\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting GPU-QuickScorer to speed-up the scoring time\n",
    "\n",
    "[GPU-QuickScorer] improves over QuickScorer by exploiting GPU multi-threading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scorer_out = !{QUICKSCORER_GPU} \\\n",
    "  -d {test_dataset_file} \\\n",
    "  -m {target_model_file} \\\n",
    "  -t 1 -l 64 -r 10 -b 4000 -y 384 -z 16384\n",
    "\n",
    "print (scorer_out.n)\n",
    "    \n",
    "# takes the scoring time in milli-seconds\n",
    "scoring_time = float(scorer_out.l[-2].split()[-2])* 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[len(results)] = ['GPU-QS', target_model.n_trees, target_model_effectiveness, scoring_time, 0]\n",
    "results[\"Speed-up\"] = results[\"Doc. Scoring Time µs.\"][0]/ results[\"Doc. Scoring Time µs.\"]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "\n",
    "### References\n",
    "\n",
    "  [LambdaMart] Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, June 2010.\n",
    "\n",
    "  [Dart] Korlakai Vinayak, R. & Gilad-Bachrach, R. (2015). DART: Dropouts meet Multiple Additive Regression Trees. Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, in PMLR 38:489-497\n",
    "\n",
    "  [XDart] Lucchese, C., Nardini, F. M., Orlando, S., Perego, R., & Trani, S. (2017). X-DART: Blending Dropout and Pruning for Efficient Learning to Rank. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 1077-1080). ACM.\n",
    "  \n",
    "  [Cleaver] Lucchese, C., Nardini, F. M., Orlando, S., Perego, R., Silvestri, F., & Trani, S. (2016). Post-learning optimization of tree ensembles for efficient ranking. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 949-952). ACM.\n",
    "  \n",
    "  [QuickScorer] Lucchese, C., Nardini, F. M., Orlando, S., Perego, R., Tonellotto, N., & Venturini, R. (2015, August). Quickscorer: A fast algorithm to rank documents with additive ensembles of regression trees. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 73-82). ACM.\n",
    "  \n",
    "  [V-QuickScorer] Lucchese, C., Nardini, F. M., Orlando, S., Perego, R., Tonellotto, N., & Venturini, R. (2016, July). Exploiting CPU SIMD extensions to speed-up document scoring with tree ensembles. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval (pp. 833-836). ACM.\n",
    "  \n",
    "  [GPU-QuickScorer] Lettich, F., Lucchese, C., Nardini, F. M., Orlando, S., Perego, R., Tonellotto, N., & Venturini, R. (2018). Parallel Traversal of Large Ensembles of Decision Trees. IEEE Transactions on Parallel and Distributed Systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
